{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68afa94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 11:19:39,203 - INFO - Starting scrape for: ceo bangalore linkedin\n",
      "2025-05-25 11:19:51,193 - INFO - Scraping page 1\n",
      "2025-05-25 11:19:51,263 - INFO - Found 11 LinkedIn links on page\n",
      "2025-05-25 11:20:01,828 - INFO - Scraped: None - https://in.linkedin.com/in/anandsriganesh#:~:text=Anand%20Sri%20Ganesh%20%2D%20Bengaluru%2C%20Karnataka,%2C%20India%20%7C%20Professional%20Profile%20%7C%20LinkedIn\n",
      "2025-05-25 11:20:12,112 - INFO - Scraped: None - https://in.linkedin.com/in/mohammed-imran-435ba0187#:~:text=Mohammed%20Imran%20%2D%20Ceo%20\n",
      "2025-05-25 11:20:25,518 - INFO - Scraped: None - https://in.linkedin.com/in/shivku#:~:text=Shivakumar%20Ganesan%20%2D%20Bengaluru%2C%20Karnataka%2C%20India%20%7C%20Professional%20Profile%20%7C%20LinkedIn\n",
      "2025-05-25 11:20:26,193 - INFO - Scraped: Nitin Gupta - Bengaluru, Karnataka, India - https://in.linkedin.com/in/nitinguptaprofile\n",
      "2025-05-25 11:20:26,707 - INFO - Scraped: Sharan Hegde - Founder & CEO - 1% Club - https://in.linkedin.com/in/sharanhegde95\n",
      "2025-05-25 11:20:27,539 - INFO - Scraped: Shivakumar Ganesan - Bengaluru, Karnataka, India - https://in.linkedin.com/in/shivku\n",
      "2025-05-25 11:20:28,658 - INFO - Scraped: Abhishek Singh - Bangalore Urban, Karnataka, India - https://in.linkedin.com/in/avicsingh01\n",
      "2025-05-25 11:20:29,295 - INFO - Scraped: Krishna Kumar Shetty - Loyola College - https://in.linkedin.com/in/krishna-kumar-shetty-1026a255\n",
      "2025-05-25 11:20:30,410 - INFO - Scraped: Yashwanth M. - Bengaluru, Karnataka, India - https://in.linkedin.com/in/yashwanthm\n",
      "2025-05-25 11:20:30,980 - INFO - Scraped: Ashish Goel - Bengaluru, Karnataka, India - https://in.linkedin.com/in/ashish-goel-591572\n",
      "2025-05-25 11:20:31,667 - INFO - Scraped: Harshil Mathur - Razorpay - https://in.linkedin.com/in/harshilmathur\n",
      "2025-05-25 11:20:31,672 - INFO - Checkpoint saved: 11 URLs\n",
      "2025-05-25 11:20:31,678 - INFO - Results saved to linkedin_ceo_bangalore_profiles.json: 11 profiles\n",
      "2025-05-25 11:20:37,112 - INFO - Scraping page 2\n",
      "2025-05-25 11:20:37,222 - INFO - Found 9 LinkedIn links on page\n",
      "2025-05-25 11:20:37,525 - INFO - Scraped: Dr.Lakshmi Jagannathan - Chief Executive Officer ... - https://in.linkedin.com/in/drlakshmijagannathan\n",
      "2025-05-25 11:20:37,880 - INFO - Scraped: Anand Sri Ganesh - Bengaluru, Karnataka, India - https://in.linkedin.com/in/anandsriganesh\n",
      "2025-05-25 11:20:38,137 - INFO - Scraped: saraswathi venkateswaran - CEO Search India Pvt Ltd - https://in.linkedin.com/in/saraswathivenkateswaran\n",
      "2025-05-25 11:20:38,416 - INFO - Scraped: Musthafa P. - Bengaluru, Karnataka, India - https://in.linkedin.com/in/musthafa-p-7052082\n",
      "2025-05-25 11:20:38,687 - INFO - Scraped: Sankar Chatterjee - Decathlon Sports India - https://in.linkedin.com/in/sankar-chatterjee-8a060614\n",
      "2025-05-25 11:20:38,982 - INFO - Scraped: Mohammed Imran - Ceo & Founder - Bangalore Job Hub - https://in.linkedin.com/in/mohammed-imran-435ba0187\n",
      "2025-05-25 11:20:39,224 - INFO - Scraped: Nandita Sinha - Myntra - https://in.linkedin.com/in/nandita-sinha-3020275\n",
      "2025-05-25 11:20:39,463 - INFO - Scraped: Amit Gupta - Yulu - https://in.linkedin.com/in/amitgupta007\n",
      "2025-05-25 11:20:39,703 - INFO - Scraped: Sriram Kuchimanchi - Founder & CEO - Smarter Dharma - https://in.linkedin.com/in/sriramkuchimanchi\n",
      "2025-05-25 11:20:46,575 - INFO - Scraping page 3\n",
      "2025-05-25 11:20:46,625 - INFO - Found 13 LinkedIn links on page\n",
      "2025-05-25 11:20:46,885 - INFO - Scraped: Mithun Srivatsa - Bengaluru, Karnataka, India - https://in.linkedin.com/in/mithunsrivatsa\n",
      "2025-05-25 11:20:47,131 - INFO - Scraped: Shashank ND - Confederation of Indian Industry - https://in.linkedin.com/in/shashanknd\n",
      "2025-05-25 11:20:47,384 - INFO - Scraped: Rajan Bajaj - slice - https://in.linkedin.com/in/rajanbajaj\n",
      "2025-05-25 11:20:47,639 - INFO - Scraped: Sudip Singh - ITC Infotech - https://in.linkedin.com/in/sudipsingh\n",
      "2025-05-25 11:20:58,269 - INFO - Scraped: None - https://in.linkedin.com/in/vinay-h-a98536201\n",
      "2025-05-25 11:21:08,952 - INFO - Scraped: None - https://in.linkedin.com/in/girishkurudi\n",
      "2025-05-25 11:21:19,421 - INFO - Scraped: None - https://in.linkedin.com/in/abhishek-shetty-7b625ab7\n",
      "2025-05-25 11:21:29,993 - INFO - Scraped: None - https://in.linkedin.com/in/hari-marar-77841a4\n",
      "2025-05-25 11:21:40,354 - INFO - Scraped: None - https://in.linkedin.com/in/dakshayani-shivapramod-31083575\n",
      "2025-05-25 11:21:46,198 - INFO - Scraping page 4\n",
      "2025-05-25 11:21:46,241 - INFO - Found 3 LinkedIn links on page\n",
      "2025-05-25 11:21:46,417 - INFO - Scraped: Nithin Kamath - Zerodha - https://in.linkedin.com/in/nithin-kamath-81136242\n",
      "2025-05-25 11:21:46,584 - INFO - Scraped: Sankeerth Reddy - City CEO - Bangalore - Flipkart - https://in.linkedin.com/in/sankeerth-bond\n",
      "2025-05-25 11:21:52,836 - INFO - Scraping page 5\n",
      "2025-05-25 11:21:52,866 - INFO - Found 2 LinkedIn links on page\n",
      "2025-05-25 11:21:53,118 - INFO - Scraped: Ayyappan R - FirstClub Technology Private Limited - https://in.linkedin.com/in/ayyappan-r\n",
      "2025-05-25 11:21:53,323 - INFO - Scraped: Binod Pawar - Founder-Director & CEO - https://in.linkedin.com/in/binod-pawar-010b98143\n",
      "2025-05-25 11:21:58,185 - INFO - Scraping page 6\n",
      "2025-05-25 11:21:58,240 - INFO - Found 1 LinkedIn links on page\n",
      "2025-05-25 11:21:58,467 - INFO - Scraped: Sudarshan Kamath - smallest.ai - https://www.linkedin.com/in/sudarshankamath\n",
      "2025-05-25 11:21:58,469 - INFO - Checkpoint saved: 34 URLs\n",
      "2025-05-25 11:21:58,474 - INFO - Results saved to linkedin_ceo_bangalore_profiles.json: 34 profiles\n",
      "2025-05-25 11:22:03,820 - INFO - Scraping page 7\n",
      "2025-05-25 11:22:03,866 - INFO - Found 2 LinkedIn links on page\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode characters in position 91-92: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 645, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1999, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8996\\816028172.py\", line 389, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8996\\816028172.py\", line 375, in main\n",
      "    profiles = scraper.run_scraping()\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8996\\816028172.py\", line 294, in run_scraping\n",
      "    page_profiles = self.scrape_google_page(driver)\n",
      "  File \"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_8996\\816028172.py\", line 220, in scrape_google_page\n",
      "    logger.info(f\"Scraped: {profile.name} - {profile.url}\")\n",
      "Message: 'Scraped: Bhabani Sankar Jena - Indo-Sakura Software Japan Ê†™Âºè ... - https://in.linkedin.com/in/bhabani-sankar-jena'\n",
      "Arguments: ()\n",
      "2025-05-25 11:22:04,094 - INFO - Scraped: Bhabani Sankar Jena - Indo-Sakura Software Japan Ê†™Âºè ... - https://in.linkedin.com/in/bhabani-sankar-jena\n",
      "2025-05-25 11:22:04,591 - INFO - Scraped: Rarathody Sukumaran - Founder & CEO - https://in.linkedin.com/in/rarathody-sukumaran-84107214\n",
      "2025-05-25 11:22:11,524 - INFO - Scraping page 8\n",
      "2025-05-25 11:22:11,592 - INFO - Found 5 LinkedIn links on page\n",
      "2025-05-25 11:22:11,848 - INFO - Scraped: Prashanth Jain - BMW Deutsche Motoren Bangalore - https://in.linkedin.com/in/prashanth-jain-083773200\n",
      "2025-05-25 11:22:12,117 - INFO - Scraped: Nagaraja S - CEO and Co- founder - https://in.linkedin.com/in/nagaraja-s-5340a241\n",
      "2025-05-25 11:22:12,372 - INFO - Scraped: Subra Krishnan - Lemnisk - https://in.linkedin.com/in/subramaniyan\n",
      "2025-05-25 11:22:12,578 - INFO - Scraped: Deepankar Das - CEO - ButtonShift - https://in.linkedin.com/in/milesdeep\n",
      "2025-05-25 11:22:12,785 - INFO - Scraped: Rajendra Joshi - Lodha - https://in.linkedin.com/in/rajendra-joshi-9b41898\n",
      "2025-05-25 11:22:18,366 - INFO - Scraping page 9\n",
      "2025-05-25 11:22:18,415 - INFO - Found 10 LinkedIn links on page\n",
      "2025-05-25 11:22:18,689 - INFO - Scraped: Savitha Reddy - inlingua Bangalore - https://in.linkedin.com/in/savithareddy\n",
      "2025-05-25 11:22:18,971 - INFO - Scraped: Vishal Dhupar - NVIDIA - https://in.linkedin.com/in/vishaldhupar\n",
      "2025-05-25 11:22:19,261 - INFO - Scraped: Vaibhav Tewari - Portea - https://in.linkedin.com/in/vaibhavtewari\n",
      "2025-05-25 11:22:19,472 - INFO - Scraped: Bino V.L - CEO - INNOVATIONS BANGALORE - https://in.linkedin.com/in/bino-v-l-77705248\n",
      "2025-05-25 11:22:19,698 - INFO - Scraped: Jeevan Dongre (JD) - antstack.io - https://in.linkedin.com/in/jeevandongre\n",
      "2025-05-25 11:22:19,961 - INFO - Scraped: Vishwas Mudagal - GoodWorkLabs - https://in.linkedin.com/in/vishwasmudagal\n",
      "2025-05-25 11:22:20,213 - INFO - Scraped: Raj Sakhumalla - CEO & Executive Director - SPL Co. - https://in.linkedin.com/in/rajsakhumalla\n",
      "2025-05-25 11:22:20,517 - INFO - Scraped: Anuj Agrawal - Zyoin Group - https://in.linkedin.com/in/anujagrawalzyoin\n",
      "2025-05-25 11:22:20,767 - INFO - Scraped: Prashanth Vasan - Co-Founder and CEO - Origin - https://in.linkedin.com/in/prashanth-vasan-b3b84985\n",
      "2025-05-25 11:22:21,014 - INFO - Scraped: Rahul Chawla - Global Velocity Ventures - https://in.linkedin.com/in/therahulchawla\n",
      "2025-05-25 11:22:25,952 - INFO - Scraping page 10\n",
      "2025-05-25 11:22:25,992 - INFO - Found 10 LinkedIn links on page\n",
      "2025-05-25 11:22:26,209 - INFO - Scraped: Rajneesh Karnatak - MD & CEO - Bank of India - https://in.linkedin.com/in/rajneeshkarnatak\n",
      "2025-05-25 11:22:26,383 - INFO - Scraped: Amit Mishra - Founder and CEO - Tripper Trails - https://in.linkedin.com/in/amit-mishra-1101b0110\n",
      "2025-05-25 11:22:26,557 - INFO - Scraped: Vikas Purohit - Bennett Coleman & Co. Ltd. (The Times of ... - https://in.linkedin.com/in/vikaspurohit\n",
      "2025-05-25 11:22:26,721 - INFO - Scraped: Rao KNM - Founder & CEO - Quick Ride - https://in.linkedin.com/in/knmrao\n",
      "2025-05-25 11:22:26,887 - INFO - Scraped: Vikram Labhe - Bengaluru, Karnataka, India - https://in.linkedin.com/in/vikramlabhe\n",
      "2025-05-25 11:22:27,064 - INFO - Scraped: Manu Saale - Mercedes-Benz Research and Development ... - https://in.linkedin.com/in/manu-saale-9487563\n",
      "2025-05-25 11:22:27,242 - INFO - Scraped: Ravi Gururaj - LeapMile Robotics - https://in.linkedin.com/in/rgururaj\n",
      "2025-05-25 11:22:27,409 - INFO - Scraped: Peyush Bansal - Lenskart.com - https://in.linkedin.com/in/peyushbansal\n",
      "2025-05-25 11:22:27,569 - INFO - Scraped: Dolly T. - CEO Forums- Bangalore and Chennai - https://in.linkedin.com/in/dolly-t-b05a80108\n",
      "2025-05-25 11:22:27,825 - INFO - Scraped: Nikhil Kamath - Co-Founder - Zerodha - https://in.linkedin.com/in/nikhilkamathcio\n",
      "2025-05-25 11:22:32,656 - INFO - Taking extended break...\n",
      "2025-05-25 11:23:10,173 - INFO - Scraping page 11\n",
      "2025-05-25 11:23:10,250 - INFO - Found 10 LinkedIn links on page\n",
      "2025-05-25 11:23:10,521 - INFO - Scraped: Manu Kumar Jain - G42 - https://ae.linkedin.com/in/manukumarjain\n",
      "2025-05-25 11:23:10,775 - INFO - Scraped: Dipesh Shah - Havells India Ltd - https://in.linkedin.com/in/dipesh007\n",
      "2025-05-25 11:23:11,215 - INFO - Scraped: Dr.Jayant Kumar (PhD) - Bengaluru, Karnataka, India - https://in.linkedin.com/in/jayantzuno\n",
      "2025-05-25 11:23:11,522 - INFO - Scraped: Siddharth Ramasubramanian - Bengaluru, Karnataka, India - https://in.linkedin.com/in/siddharthvaikunth\n",
      "2025-05-25 11:23:11,740 - INFO - Scraped: Vaibhav Gupta - Bengaluru, Karnataka, India - https://in.linkedin.com/in/vaibhav-gupta-629568\n",
      "2025-05-25 11:23:12,130 - INFO - Scraped: Lalit Keshre - Groww, India - https://in.linkedin.com/in/lalitkeshre\n",
      "2025-05-25 11:23:12,363 - INFO - Scraped: Shantanu Deshpande - Bombay Shaving Company - https://in.linkedin.com/in/shantanudeshpandebsc\n",
      "2025-05-25 11:23:12,589 - INFO - Scraped: Rajesh Krishnan - United Way of Bengaluru - https://in.linkedin.com/in/thondukulam\n",
      "2025-05-25 11:23:12,805 - INFO - Scraped: Amit Kumar Agarwal - NoBroker.com - https://in.linkedin.com/in/amit-kumar-agarwal-5b30301\n",
      "2025-05-25 11:23:12,808 - INFO - Checkpoint saved: 70 URLs\n",
      "2025-05-25 11:23:12,813 - INFO - Results saved to linkedin_ceo_bangalore_profiles.json: 70 profiles\n",
      "2025-05-25 11:23:18,850 - INFO - Scraping page 12\n",
      "2025-05-25 11:23:18,893 - INFO - Found 8 LinkedIn links on page\n",
      "2025-05-25 11:23:19,145 - INFO - Scraped: Guruprasad Mudlapur - Bosch India - https://in.linkedin.com/in/gmudlapur\n",
      "2025-05-25 11:23:19,379 - INFO - Scraped: Nirupesh Joshi - Bangalore Watch Company - https://in.linkedin.com/in/nirupesh\n",
      "2025-05-25 11:23:19,623 - INFO - Scraped: Preeyanka Sengupta - Ceo - Indian Institute of ... - https://in.linkedin.com/in/preeyanka-sengupta-3249b085\n",
      "2025-05-25 11:23:19,866 - INFO - Scraped: Harsh Vardhan Singh - Founder & CEO - Vikgol - https://in.linkedin.com/in/harsh-vardhan-singh-dscet\n",
      "2025-05-25 11:23:20,110 - INFO - Scraped: Anil K. - Chief Executive Officer - Menzies Aviation - https://in.linkedin.com/in/anil-k-81814634\n",
      "2025-05-25 11:23:20,453 - INFO - Scraped: Chandrasekhar K - Bengaluru, Karnataka, India - https://in.linkedin.com/in/kcsekhar\n",
      "2025-05-25 11:23:20,697 - INFO - Scraped: David Rajesh - Bangalore University - https://in.linkedin.com/in/david-rajesh-108b0086\n",
      "2025-05-25 11:23:20,970 - INFO - Scraped: Soumitra Bhattacharya - Indian Foundation for Quality ... - https://in.linkedin.com/in/soumitra-bhattacharya-8ba83b184\n",
      "2025-05-25 11:23:25,954 - INFO - Scraping page 13\n",
      "2025-05-25 11:23:25,998 - INFO - Found 10 LinkedIn links on page\n",
      "2025-05-25 11:23:26,383 - INFO - Scraped: Hari Menon - Co-Founder & CEO - bigbasket.com - https://in.linkedin.com/in/harimenon-bigbasket\n",
      "2025-05-25 11:23:26,620 - INFO - Scraped: Prashant Gupta - Bengaluru, Karnataka, India - https://in.linkedin.com/in/prashant-gupta-bandhoo\n",
      "2025-05-25 11:23:26,872 - INFO - Scraped: Pankaj Vyas - Siemens Technology India - https://in.linkedin.com/in/pankajnvyas\n",
      "2025-05-25 11:23:27,091 - INFO - Scraped: Pratik Kumar - Bangalore Rural, Karnataka, India - https://in.linkedin.com/in/pratik-kumar-2a819b3\n",
      "2025-05-25 11:23:27,332 - INFO - Scraped: Abhishek Poddar - Plum - https://in.linkedin.com/in/abhishek24\n",
      "2025-05-25 11:23:27,597 - INFO - Scraped: Nair Vineeth - AJIO.com - https://in.linkedin.com/in/nair-vineeth-91b528b\n",
      "2025-05-25 11:23:27,826 - INFO - Scraped: Sachin Bansal - Navi - https://in.linkedin.com/in/sachinbansal\n",
      "2025-05-25 11:23:28,058 - INFO - Scraped: Sidhavelayutham Mohanamoorthy - Alice Blue - https://in.linkedin.com/in/sidhavelayutham-mohanamoorthy-20981739\n",
      "2025-05-25 11:23:34,033 - INFO - Scraping page 14\n",
      "2025-05-25 11:23:34,098 - INFO - Found 9 LinkedIn links on page\n",
      "2025-05-25 11:23:34,346 - INFO - Scraped: Sunil Gopinath - Rakuten - https://in.linkedin.com/in/sunilgopinath\n",
      "2025-05-25 11:23:34,601 - INFO - Scraped: Sandeep Dhar - Wipro - https://in.linkedin.com/in/sandeepdhar\n",
      "2025-05-25 11:23:34,909 - INFO - Scraped: Kumar Venkatasubramanian - Procter & Gamble - https://in.linkedin.com/in/kumar-venkatasubramanian\n",
      "2025-05-25 11:23:35,137 - INFO - Scraped: Dhananjay Mishra - Co-Founder & CEO - TruEstate - https://in.linkedin.com/in/dhananjay-mishra-346a7620\n",
      "2025-05-25 11:23:35,355 - INFO - Scraped: Ravi Kumar S - Cognizant - https://www.linkedin.com/in/imravikumars\n",
      "2025-05-25 11:23:35,588 - INFO - Scraped: Karan Bhagat - Founder MD & CEO - 360 ONE - https://in.linkedin.com/in/karanbhagat360one\n",
      "2025-05-25 11:23:35,820 - INFO - Scraped: Ankur Warikoo - Founder - WebVeda - https://in.linkedin.com/in/warikoo\n",
      "2025-05-25 11:23:36,059 - INFO - Scraped: Amit Mishra - Founder & CEO - Dazeinfo Media & Research - https://in.linkedin.com/in/amit6060\n",
      "2025-05-25 11:23:36,307 - INFO - Scraped: Anil Agarwal - CEO & Co-Founder - InCruiter - https://in.linkedin.com/in/anil-incruiter\n",
      "2025-05-25 11:23:43,387 - INFO - Scraping page 15\n",
      "2025-05-25 11:23:43,501 - INFO - Found 4 LinkedIn links on page\n",
      "2025-05-25 11:23:43,753 - INFO - Scraped: Suresh Rangarajan - Bengaluru, Karnataka, India - https://in.linkedin.com/in/sureshrangaraj\n",
      "2025-05-25 11:23:44,005 - INFO - Scraped: Arindam Banerrji - Bengaluru, Karnataka, India - https://in.linkedin.com/in/arindam-banerrji-ab62a1\n",
      "2025-05-25 11:23:44,243 - INFO - Scraped: Vipul Parekh - Bengaluru, Karnataka, India - https://in.linkedin.com/in/vipulparekh\n",
      "2025-05-25 11:23:44,490 - INFO - Scraped: Ananth Narayanan - Founder - Mensa Brands - https://in.linkedin.com/in/ananth-narayanan1\n",
      "2025-05-25 11:24:24,607 - INFO - No more pages available\n",
      "2025-05-25 11:24:27,016 - INFO - Checkpoint saved: 99 URLs\n",
      "2025-05-25 11:24:27,024 - INFO - Results saved to linkedin_ceo_bangalore_profiles.json: 99 profiles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCRAPING SUMMARY\n",
      "==================================================\n",
      "Search Query: ceo bangalore linkedin\n",
      "Total Profiles Found: 99\n",
      "Unique URLs Scraped: 99\n",
      "Output File: linkedin_ceo_bangalore_profiles.json\n",
      "Checkpoint File: scraped_urls_ceo_bangalore.json\n",
      "\n",
      "Sample Profiles:\n",
      "1. None - None (bangalore)\n",
      "   URL: https://in.linkedin.com/in/anandsriganesh#:~:text=Anand%20Sri%20Ganesh%20%2D%20Bengaluru%2C%20Karnataka,%2C%20India%20%7C%20Professional%20Profile%20%7C%20LinkedIn\n",
      "2. None - None (bangalore)\n",
      "   URL: https://in.linkedin.com/in/mohammed-imran-435ba0187#:~:text=Mohammed%20Imran%20%2D%20Ceo%20\n",
      "3. None - None (bangalore)\n",
      "   URL: https://in.linkedin.com/in/shivku#:~:text=Shivakumar%20Ganesan%20%2D%20Bengaluru%2C%20Karnataka%2C%20India%20%7C%20Professional%20Profile%20%7C%20LinkedIn\n",
      "4. Nitin Gupta - Bengaluru, Karnataka, India - None (bangalore)\n",
      "   URL: https://in.linkedin.com/in/nitinguptaprofile\n",
      "5. Sharan Hegde - Founder & CEO - 1% Club - None (bangalore)\n",
      "   URL: https://in.linkedin.com/in/sharanhegde95\n",
      "==================================================\n",
      "\n",
      "‚úÖ Successfully scraped 99 LinkedIn profiles!\n",
      "üìÅ Results saved to: linkedin_ceo_bangalore_profiles.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LinkedIn Profile Scraper via Google Search\n",
    "Automated discovery of CEO/CFO LinkedIn profiles by city\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import Dict, List, Set, Optional\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException, \n",
    "    WebDriverException, \n",
    "    TimeoutException,\n",
    "    ElementClickInterceptedException\n",
    ")\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class LinkedInProfile:\n",
    "    \"\"\"Data structure for LinkedIn profile information\"\"\"\n",
    "    name: Optional[str]\n",
    "    url: str\n",
    "    city: Optional[str]\n",
    "    company: Optional[str]\n",
    "    title: Optional[str]\n",
    "    scraped_at: str\n",
    "\n",
    "class LinkedInScraper:\n",
    "    \"\"\"Main scraper class for LinkedIn profiles via Google Search\"\"\"\n",
    "    \n",
    "    def __init__(self, position: str = \"ceo\", city: str = \"bangalore\"):\n",
    "        self.position = position.lower()\n",
    "        self.city = city.lower()\n",
    "        self.search_query = f\"{position} {city} linkedin\"\n",
    "        self.max_pages = 30\n",
    "        self.checkpoint_file = f\"scraped_urls_{position}_{city}.json\"\n",
    "        self.output_file = f\"linkedin_{position}_{city}_profiles.json\"\n",
    "        self.scraped_urls: Set[str] = set()\n",
    "        self.profiles: List[LinkedInProfile] = []\n",
    "        \n",
    "        # User agents for rotation\n",
    "        self.user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "        ]\n",
    "    \n",
    "    def get_stealth_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Create a stealth Chrome driver to avoid detection\"\"\"\n",
    "        options = Options()\n",
    "        \n",
    "        # Stealth settings\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--disable-plugins\")\n",
    "        options.add_argument(\"--disable-images\")  # Faster loading\n",
    "        options.add_argument(\"--disable-javascript\")  # For basic scraping\n",
    "        options.add_argument(f\"--user-agent={random.choice(self.user_agents)}\")\n",
    "        \n",
    "        # Exclude automation switches\n",
    "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # Additional prefs\n",
    "        prefs = {\n",
    "            \"profile.default_content_setting_values\": {\n",
    "                \"notifications\": 2,\n",
    "                \"media_stream\": 2,\n",
    "            }\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "        try:\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.set_page_load_timeout(30)\n",
    "            driver.implicitly_wait(10)\n",
    "            \n",
    "            # Execute script to remove webdriver property\n",
    "            driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            return driver\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to create driver: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_checkpoint(self) -> None:\n",
    "        \"\"\"Load previously scraped URLs from checkpoint file\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    self.scraped_urls = set(data.get(\"urls\", []))\n",
    "                    logger.info(f\"Loaded {len(self.scraped_urls)} previously scraped URLs\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load checkpoint: {e}\")\n",
    "                self.scraped_urls = set()\n",
    "    \n",
    "    def save_checkpoint(self) -> None:\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        try:\n",
    "            with open(self.checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"urls\": list(self.scraped_urls),\n",
    "                    \"total_profiles\": len(self.profiles),\n",
    "                    \"last_updated\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                }, f, indent=2)\n",
    "            logger.info(f\"Checkpoint saved: {len(self.scraped_urls)} URLs\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "    \n",
    "    def human_delay(self, min_delay: float = 2.0, max_delay: float = 5.0) -> None:\n",
    "        \"\"\"Simulate human-like delays\"\"\"\n",
    "        delay = random.uniform(min_delay, max_delay)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def extract_linkedin_data(self, link_element, driver) -> Optional[LinkedInProfile]:\n",
    "        \"\"\"Extract LinkedIn profile data from search result element\"\"\"\n",
    "        try:\n",
    "            # Get URL\n",
    "            url = link_element.get_attribute('href')\n",
    "            if not url or 'linkedin.com/in/' not in url:\n",
    "                return None\n",
    "            \n",
    "            # Clean URL\n",
    "            url = url.split('&')[0].split('?')[0]\n",
    "            \n",
    "            # Skip if already scraped\n",
    "            if url in self.scraped_urls:\n",
    "                return None\n",
    "            \n",
    "            # Extract name from title/text\n",
    "            name = None\n",
    "            try:\n",
    "                h3_element = link_element.find_element(By.TAG_NAME, \"h3\")\n",
    "                name = h3_element.text.strip()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Extract additional info from description\n",
    "            city, company, title = None, None, None\n",
    "            try:\n",
    "                parent = link_element.find_element(By.XPATH, \"./..\")\n",
    "                description_elements = parent.find_elements(By.CSS_SELECTOR, \"span, div\")\n",
    "                \n",
    "                for elem in description_elements:\n",
    "                    text = elem.text.strip()\n",
    "                    if text and len(text) > 3:\n",
    "                        # Try to identify city, company, or title\n",
    "                        if any(word in text.lower() for word in [self.city, 'ceo', 'cfo', 'president', 'director']):\n",
    "                            if not title and any(word in text.lower() for word in ['ceo', 'cfo', 'president', 'director']):\n",
    "                                title = text\n",
    "                            elif not city and self.city.lower() in text.lower():\n",
    "                                city = text\n",
    "                            elif not company and len(text.split()) <= 4:\n",
    "                                company = text\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            profile = LinkedInProfile(\n",
    "                name=name,\n",
    "                url=url,\n",
    "                city=city or self.city,\n",
    "                company=company,\n",
    "                title=title or self.position.upper(),\n",
    "                scraped_at=time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            \n",
    "            self.scraped_urls.add(url)\n",
    "            return profile\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to extract profile data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_google_page(self, driver) -> List[LinkedInProfile]:\n",
    "        \"\"\"Scrape LinkedIn profiles from current Google results page\"\"\"\n",
    "        profiles = []\n",
    "        \n",
    "        try:\n",
    "            # Wait for results to load\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div#search\"))\n",
    "            )\n",
    "            \n",
    "            # Find LinkedIn links\n",
    "            linkedin_links = driver.find_elements(By.XPATH, \"//a[contains(@href, 'linkedin.com/in/')]\")\n",
    "            logger.info(f\"Found {len(linkedin_links)} LinkedIn links on page\")\n",
    "            \n",
    "            for link in linkedin_links:\n",
    "                try:\n",
    "                    profile = self.extract_linkedin_data(link, driver)\n",
    "                    if profile:\n",
    "                        profiles.append(profile)\n",
    "                        logger.info(f\"Scraped: {profile.name} - {profile.url}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error processing link: {e}\")\n",
    "                    continue\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timeout waiting for search results\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping page: {e}\")\n",
    "        \n",
    "        return profiles\n",
    "    \n",
    "    def navigate_to_next_page(self, driver) -> bool:\n",
    "        \"\"\"Navigate to next Google results page\"\"\"\n",
    "        try:\n",
    "            # Look for next button\n",
    "            next_selectors = [\n",
    "                \"a#pnnext\",\n",
    "                \"a[aria-label='Next']\",\n",
    "                \"a[aria-label*='Next']\",\n",
    "                \"//a[contains(text(), 'Next')]\"\n",
    "            ]\n",
    "            \n",
    "            for selector in next_selectors:\n",
    "                try:\n",
    "                    if selector.startswith(\"//\"):\n",
    "                        next_button = driver.find_element(By.XPATH, selector)\n",
    "                    else:\n",
    "                        next_button = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                    \n",
    "                    if next_button.is_enabled():\n",
    "                        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                        self.human_delay(3, 6)\n",
    "                        return True\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to navigate to next page: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_scraping(self) -> List[LinkedInProfile]:\n",
    "        \"\"\"Main scraping execution\"\"\"\n",
    "        logger.info(f\"Starting scrape for: {self.search_query}\")\n",
    "        \n",
    "        # Load previous progress\n",
    "        self.load_checkpoint()\n",
    "        \n",
    "        driver = None\n",
    "        try:\n",
    "            driver = self.get_stealth_driver()\n",
    "            \n",
    "            # Go to Google\n",
    "            driver.get(\"https://www.google.com\")\n",
    "            self.human_delay()\n",
    "            \n",
    "            # Perform search\n",
    "            search_box = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.NAME, \"q\"))\n",
    "            )\n",
    "            search_box.clear()\n",
    "            search_box.send_keys(self.search_query)\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            \n",
    "            self.human_delay(2, 4)\n",
    "            \n",
    "            # Scrape pages\n",
    "            page_count = 0\n",
    "            while page_count < self.max_pages:\n",
    "                logger.info(f\"Scraping page {page_count + 1}\")\n",
    "                \n",
    "                # Scrape current page\n",
    "                page_profiles = self.scrape_google_page(driver)\n",
    "                self.profiles.extend(page_profiles)\n",
    "                \n",
    "                # Save progress\n",
    "                if page_count % 5 == 0:  # Save every 5 pages\n",
    "                    self.save_checkpoint()\n",
    "                    self.save_results()\n",
    "                \n",
    "                # Navigate to next page\n",
    "                if not self.navigate_to_next_page(driver):\n",
    "                    logger.info(\"No more pages available\")\n",
    "                    break\n",
    "                \n",
    "                page_count += 1\n",
    "                \n",
    "                # Longer delay every few pages to avoid detection\n",
    "                if page_count % 10 == 0:\n",
    "                    logger.info(\"Taking extended break...\")\n",
    "                    self.human_delay(30, 60)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Scraping interrupted by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Scraping failed: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "            \n",
    "            # Final save\n",
    "            self.save_checkpoint()\n",
    "            self.save_results()\n",
    "        \n",
    "        return self.profiles\n",
    "    \n",
    "    def save_results(self) -> None:\n",
    "        \"\"\"Save scraped profiles to JSON file\"\"\"\n",
    "        try:\n",
    "            profiles_data = [asdict(profile) for profile in self.profiles]\n",
    "            \n",
    "            with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\n",
    "                    \"search_query\": self.search_query,\n",
    "                    \"total_profiles\": len(profiles_data),\n",
    "                    \"scraped_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"profiles\": profiles_data\n",
    "                }, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"Results saved to {self.output_file}: {len(profiles_data)} profiles\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save results: {e}\")\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"Print scraping summary\"\"\"\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"SCRAPING SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Search Query: {self.search_query}\")\n",
    "        print(f\"Total Profiles Found: {len(self.profiles)}\")\n",
    "        print(f\"Unique URLs Scraped: {len(self.scraped_urls)}\")\n",
    "        print(f\"Output File: {self.output_file}\")\n",
    "        print(f\"Checkpoint File: {self.checkpoint_file}\")\n",
    "        \n",
    "        if self.profiles:\n",
    "            print(f\"\\nSample Profiles:\")\n",
    "            for i, profile in enumerate(self.profiles[:5]):\n",
    "                print(f\"{i+1}. {profile.name} - {profile.company} ({profile.city})\")\n",
    "                print(f\"   URL: {profile.url}\")\n",
    "        \n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Configuration\n",
    "    POSITION = \"ceo\"  # Change as needed: ceo, cfo, president, etc.\n",
    "    CITY = \"bangalore\"  # Change as needed: bangalore, mumbai, delhi, etc.\n",
    "    \n",
    "    # Create and run scraper\n",
    "    scraper = LinkedInScraper(position=POSITION, city=CITY)\n",
    "    \n",
    "    try:\n",
    "        profiles = scraper.run_scraping()\n",
    "        scraper.print_summary()\n",
    "        \n",
    "        if profiles:\n",
    "            print(f\"‚úÖ Successfully scraped {len(profiles)} LinkedIn profiles!\")\n",
    "            print(f\"üìÅ Results saved to: {scraper.output_file}\")\n",
    "        else:\n",
    "            print(\"‚ùå No profiles found. Check your search parameters or try again later.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {e}\")\n",
    "        print(f\"‚ùå Scraping failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
